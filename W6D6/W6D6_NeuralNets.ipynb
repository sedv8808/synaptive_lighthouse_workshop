{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lighthouse Labs - Synaptive Medical\n",
    "\n",
    "### W6D6 Neural Networks\n",
    "\n",
    "Instructor: Socorro Dominguez  \n",
    "January 08, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Agenda:**\n",
    "- Review: What is DL, comparison to classic ML\n",
    "- Backpropagation\n",
    "- Neural network - Demo\n",
    "    - Epochs and batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Deep Learning** is a subfield of *machine learning* concerned with algorithms inspired by the structure and function of the brain called artificial neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![img](img/neural_nets.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why Deep Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![img](img/unstructured_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Review: What is a Neural Network?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We define the function recursively:\n",
    "\n",
    "$$ x^{(l+1)} = h\\left( W^{(l)} x^{(l)} + b^{(l)}\\right) $$\n",
    "\n",
    "where $W^{(l)}$ is a matrix of parameters, $b^{(l)}$ is a vector of parameters. \n",
    "\n",
    "So what is $x^{(l)}$?\n",
    " * $x^{(0)}$ are the inputs\n",
    " * $x^{(L)}$ are the outputs, so we can say $\\hat{y}=x^{(L)}$\n",
    " * we refer to $L-1$ as the _number of hidden layers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Also: \n",
    " - the $W^{(l)}$ do _not_ need to be square. \n",
    " - the $x^{(l)}$ for $0<l<L$ are \"intermediate states\"\n",
    "   - there are called _hidden units_ or _hidden neurons_\n",
    "   - the _values_ of these units are called _activations_\n",
    " - we often refer to the elements of $W$ as \"weights\" and the elements of $b$ as \"biases\"\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg)\n",
    "\n",
    "In the diagrams above, circles are states and arrows carry weights.\n",
    "\n",
    "Important note: neural nets map from $\\mathbb{R}^d\\rightarrow \\mathbb{R}^k$ for some arbitrary $d$ and $k$. The outputs do not have to be scalars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activation functions\n",
    "\n",
    " - $h$ is called the _activation function_. \n",
    " - Question: why do we need $h$ at all?\n",
    " - Answer: if no $h$, then we are composing a bunch of linear functions, which just leaves us with a linear function.\n",
    " - Insight: if $h$ is nonlinear, then increasing the number of \"layers\" increases the complexity of the overall function. \n",
    " \n",
    "In neural networks, we choose $h$ to be an _elementwise_ nonlinear function. i.e.\n",
    "\n",
    "$$h(x)\\equiv\\left[\\begin{array}{c}h(x_1)\\\\h(x_2)\\\\ \\vdots \\\\ h(x_d)  \\end{array}\\right]$$\n",
    "\n",
    "Activation functions tend to be continuous, but are [not always smooth or monotonic](https://arxiv.org/pdf/1710.05941.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Batch\n",
    "\n",
    "What is a batch?\n",
    "\n",
    "The *batch size* is a hyperparameter that defines the number of samples to work through before updating the internal model parameters.\n",
    "\n",
    "Think of a batch as a for-loop iterating over one or more samples and making predictions. At the end of the batch, the predictions are compared to the expected output variables and an error is calculated. From this error, the update algorithm is used to improve the model, e.g. move down along the error gradient.\n",
    "\n",
    "A training dataset can be divided into one or more batches.\n",
    "\n",
    "**Batch Gradient Descent.** Batch Size = Size of Training Set  \n",
    "**Stochastic Gradient Descent.** Batch Size = 1  \n",
    "**Mini-Batch Gradient Descent.** 1 < Batch Size < Size of Training Set  \n",
    ">    In mini-batch GD, popular batch sizes are 32, 64, and 128 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Epochs\n",
    "\n",
    "What are epochs?\n",
    "\n",
    "- An epoch is an entire pass through the training set.\n",
    "- With minibatch size of 1, an epoch is `n` iterations.\n",
    "- With a general minibatch size, \n",
    "\n",
    "$\\text{epochs} = \\frac{\\text{iterations}}{n}\\times \\text{batch size}$\n",
    "\n",
    "Example: if the dataset has $100,000$ examples and your minibatch size is $1000$, then an epoch is $100$ iterations of stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Differences or Batches and Epochs\n",
    "\n",
    "* The number of epochs is the number of complete passes through the training dataset.\n",
    "\n",
    "* The size of a batch must be more than or equal to one and less than or equal to the number of samples in the training dataset.\n",
    "\n",
    "* The number of epochs can be set to an integer value between one and infinity. You can run the algorithm for as long as you like and even stop it using other criteria besides a fixed number of epochs, such as a change (or lack of change) in model error over time.\n",
    "\n",
    "* Both are integer values and they are both hyperparameters for the learning algorithm, e.g. parameters for the learning process, not internal model parameters found by the learning process.\n",
    "\n",
    "* You must specify the batch size and number of epochs for a learning algorithm.\n",
    "\n",
    "* There are no magic rules for how to configure these parameters. You must try different values and see what works best for your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![img](img/lossfunc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](img/GD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](img/GD2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Backpropagation\n",
    "\n",
    "\n",
    "The squared loss (a not-uncommon choice for regression) is\n",
    "\n",
    "$$f\\left(\\{W^{(l)}\\}\\right)= \\sum_{i=1}^n \\left(y_i - \\hat{y}_i\\right)^2  $$\n",
    "\n",
    "Let's digest this...\n",
    "\n",
    "- by $\\{W^{(l)}\\}$ I mean the set of all $W$'s and all their elements\n",
    "- by $\\hat{y}_i$ I mean our prediction for example $x_i$, which we get from applying our recurrence relation $L$ times.\n",
    "\n",
    "We need $$\\frac{df}{dW}$$\n",
    "\n",
    "This is done via the chain rule. But we need to be careful not to _recompute_ things (remember dynamic programming?? it was all about not recomputing things). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can draw a graph of what depends on what. Consider $\\frac{\\partial f}{\\partial W^{(0)}_{11}}$ and $\\frac{\\partial f}{\\partial W^{(0)}_{12}}$. These two derivatives have a lot in commmon, namely...\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial x^{(L)}} \\frac{\\partial x^{(L)}}{\\partial x^{(L-1)}} \\cdots \\frac{\\partial x^{(2)}}{\\partial x^{(1)}} \\frac{\\partial x^{(1)}}{\\partial W^{(0)}}$$ \n",
    "\n",
    "only the last part is different.\n",
    "\n",
    "- The method for applying the chain rule and not re-computing anything is called **backpropagation** or backprop for short. \n",
    "- Backprop is reverse-mode differentiation. So packages like AutoGrad do it \"for free\".\n",
    "- Once we have the gradient, we can train with (stochastic) gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![img](img/GD3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](img/Backprop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wage_per_hour</th>\n",
       "      <th>union</th>\n",
       "      <th>education_yrs</th>\n",
       "      <th>experience_yrs</th>\n",
       "      <th>age</th>\n",
       "      <th>female</th>\n",
       "      <th>marr</th>\n",
       "      <th>south</th>\n",
       "      <th>manufacturing</th>\n",
       "      <th>construction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.10</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.95</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>42</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.67</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.00</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.50</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   wage_per_hour  union  education_yrs  experience_yrs  age  female  marr  \\\n",
       "0           5.10      0              8              21   35       1     1   \n",
       "1           4.95      0              9              42   57       1     1   \n",
       "2           6.67      0             12               1   19       0     0   \n",
       "3           4.00      0             12               4   22       0     0   \n",
       "4           7.50      0             12              17   35       0     1   \n",
       "\n",
       "   south  manufacturing  construction  \n",
       "0      0              1             0  \n",
       "1      0              1             0  \n",
       "2      0              1             0  \n",
       "3      0              0             0  \n",
       "4      0              0             0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/hourly_wages_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  train_test_split\n",
    "\n",
    "X = df.drop(columns=['wage_per_hour'])\n",
    "y = df[['wage_per_hour']]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Train a multi-layer perceptron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 122.9703 - val_loss: 116.2418\n",
      "Epoch 2/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 106.7502 - val_loss: 101.7117\n",
      "Epoch 3/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 93.6266 - val_loss: 88.5609\n",
      "Epoch 4/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 80.6830 - val_loss: 74.8380\n",
      "Epoch 5/300\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 67.5952 - val_loss: 60.7551\n",
      "Epoch 6/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 54.6583 - val_loss: 48.2725\n",
      "Epoch 7/300\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 43.8722 - val_loss: 38.5113\n",
      "Epoch 8/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 36.2418 - val_loss: 32.1830\n",
      "Epoch 9/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 31.7199 - val_loss: 28.7188\n",
      "Epoch 10/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 29.6898 - val_loss: 27.2239\n",
      "Epoch 11/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 28.9898 - val_loss: 26.6441\n",
      "Epoch 12/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 28.7715 - val_loss: 26.4361\n",
      "Epoch 13/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 28.6334 - val_loss: 26.3049\n",
      "Epoch 14/300\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 28.4860 - val_loss: 26.1626\n",
      "Epoch 15/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 28.3342 - val_loss: 26.0043\n",
      "Epoch 16/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 28.2161 - val_loss: 25.9398\n",
      "Epoch 17/300\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 28.0479 - val_loss: 25.8204\n",
      "Epoch 18/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 27.8985 - val_loss: 25.6551\n",
      "Epoch 19/300\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 27.7634 - val_loss: 25.5321\n",
      "Epoch 20/300\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 27.6194 - val_loss: 25.2945\n",
      "Epoch 21/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 27.4543 - val_loss: 25.1456\n",
      "Epoch 22/300\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 27.3183 - val_loss: 25.0345\n",
      "Epoch 23/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 27.1695 - val_loss: 24.9263\n",
      "Epoch 24/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 27.0624 - val_loss: 24.8123\n",
      "Epoch 25/300\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 26.9210 - val_loss: 24.6281\n",
      "Epoch 26/300\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 26.7826 - val_loss: 24.4966\n",
      "Epoch 27/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 26.6566 - val_loss: 24.3500\n",
      "Epoch 28/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 26.5125 - val_loss: 24.2365\n",
      "Epoch 29/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 26.4206 - val_loss: 24.0926\n",
      "Epoch 30/300\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 26.2574 - val_loss: 24.0179\n",
      "Epoch 31/300\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 26.1650 - val_loss: 23.9792\n",
      "Epoch 32/300\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 26.0392 - val_loss: 23.7403\n",
      "Epoch 33/300\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 25.9197 - val_loss: 23.5777\n",
      "Epoch 34/300\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 25.7622 - val_loss: 23.4850\n",
      "Epoch 35/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 25.6392 - val_loss: 23.4101\n",
      "Epoch 36/300\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 25.5314 - val_loss: 23.2968\n",
      "Epoch 37/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 25.4075 - val_loss: 23.1071\n",
      "Epoch 38/300\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 25.3070 - val_loss: 22.9865\n",
      "Epoch 39/300\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 25.1978 - val_loss: 22.8810\n",
      "Epoch 40/300\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 25.0682 - val_loss: 22.7960\n",
      "Epoch 41/300\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 24.9944 - val_loss: 22.7360\n",
      "Epoch 42/300\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 24.8867 - val_loss: 22.6263\n",
      "Epoch 43/300\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 24.7624 - val_loss: 22.4943\n",
      "Epoch 44/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 24.6826 - val_loss: 22.3487\n",
      "Epoch 45/300\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 24.5542 - val_loss: 22.2767\n",
      "Epoch 46/300\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 24.4835 - val_loss: 22.2000\n",
      "Epoch 47/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 24.4104 - val_loss: 22.0730\n",
      "Epoch 48/300\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 24.3101 - val_loss: 21.9855\n",
      "Epoch 49/300\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 24.1995 - val_loss: 21.9853\n",
      "Epoch 50/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 24.1885 - val_loss: 21.9466\n",
      "Epoch 51/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 24.0569 - val_loss: 21.7402\n",
      "Epoch 52/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 23.9593 - val_loss: 21.6754\n",
      "Epoch 53/300\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 23.9128 - val_loss: 21.5779\n",
      "Epoch 54/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 23.8122 - val_loss: 21.5597\n",
      "Epoch 55/300\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 23.7322 - val_loss: 21.4792\n",
      "Epoch 56/300\n",
      "14/14 [==============================] - ETA: 0s - loss: 19.89 - 0s 4ms/step - loss: 23.6678 - val_loss: 21.4021\n",
      "Epoch 57/300\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 23.6017 - val_loss: 21.3248\n",
      "Epoch 58/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 23.5267 - val_loss: 21.2952\n",
      "Epoch 59/300\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 23.4614 - val_loss: 21.2247\n",
      "Epoch 60/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 23.4028 - val_loss: 21.1516\n",
      "Epoch 61/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 23.3500 - val_loss: 21.1202\n",
      "Epoch 62/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 23.2862 - val_loss: 21.0865\n",
      "Epoch 63/300\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 23.2525 - val_loss: 21.0336\n",
      "Epoch 64/300\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 23.1698 - val_loss: 20.9262\n",
      "Epoch 65/300\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 23.1202 - val_loss: 20.8250\n",
      "Epoch 66/300\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 23.1041 - val_loss: 20.8043\n",
      "Epoch 67/300\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 23.2294 - val_loss: 20.7150\n",
      "Epoch 68/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.9708 - val_loss: 20.7876\n",
      "Epoch 69/300\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 22.9492 - val_loss: 20.7235\n",
      "Epoch 70/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.8987 - val_loss: 20.6691\n",
      "Epoch 71/300\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 22.8523 - val_loss: 20.5858\n",
      "Epoch 72/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.8286 - val_loss: 20.5612\n",
      "Epoch 73/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.8098 - val_loss: 20.5052\n",
      "Epoch 74/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 22.7359 - val_loss: 20.5322\n",
      "Epoch 75/300\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 22.7646 - val_loss: 20.5927\n",
      "Epoch 76/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.6924 - val_loss: 20.4932\n",
      "Epoch 77/300\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 22.7348 - val_loss: 20.3597\n",
      "Epoch 78/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 22.6974 - val_loss: 20.4151\n",
      "Epoch 79/300\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 22.6157 - val_loss: 20.3097\n",
      "Epoch 80/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 22.5756 - val_loss: 20.3200\n",
      "Epoch 81/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 4ms/step - loss: 22.5395 - val_loss: 20.2887\n",
      "Epoch 82/300\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 22.5206 - val_loss: 20.2638\n",
      "Epoch 83/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 22.5016 - val_loss: 20.2173\n",
      "Epoch 84/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 22.4955 - val_loss: 20.2446\n",
      "Epoch 85/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 22.4604 - val_loss: 20.1870\n",
      "Epoch 86/300\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 22.4445 - val_loss: 20.2267\n",
      "Epoch 87/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 22.4358 - val_loss: 20.1653\n",
      "Epoch 88/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 22.4611 - val_loss: 20.2447\n",
      "Epoch 89/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 22.3741 - val_loss: 20.1099\n",
      "Epoch 90/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 22.3488 - val_loss: 20.1054\n",
      "Epoch 91/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 22.3541 - val_loss: 20.1376\n",
      "Epoch 92/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 22.3863 - val_loss: 20.0439\n",
      "Epoch 93/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 22.3852 - val_loss: 20.0080\n",
      "Epoch 94/300\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 22.2743 - val_loss: 20.0892\n",
      "Epoch 95/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.2863 - val_loss: 20.0342\n",
      "Epoch 96/300\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 22.2541 - val_loss: 20.0337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd426077690>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential        # Helper to build a network from a sequence of layers\n",
    "from tensorflow.keras.layers import Dense             # Fully-connected layer\n",
    "from tensorflow.keras.callbacks import EarlyStopping  # To stop training early if val loss stops decreasing\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', input_shape=(X.shape[1],)))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Train the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')    # Builds the static computation graph\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=300, batch_size=32, \n",
    "          callbacks=[EarlyStopping(patience=3)], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Comparison to linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 102.7585 - val_loss: 95.3051\n",
      "Epoch 2/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 86.6366 - val_loss: 79.7631\n",
      "Epoch 3/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 72.9925 - val_loss: 66.8650\n",
      "Epoch 4/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 61.9205 - val_loss: 56.4370\n",
      "Epoch 5/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 53.0640 - val_loss: 48.0060\n",
      "Epoch 6/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 46.1385 - val_loss: 41.4791\n",
      "Epoch 7/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 40.8426 - val_loss: 36.6299\n",
      "Epoch 8/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 36.9895 - val_loss: 33.0835\n",
      "Epoch 9/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 34.1872 - val_loss: 30.3787\n",
      "Epoch 10/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 32.1527 - val_loss: 28.4209\n",
      "Epoch 11/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 30.7123 - val_loss: 27.0791\n",
      "Epoch 12/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 29.7810 - val_loss: 26.0200\n",
      "Epoch 13/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 29.0595 - val_loss: 25.3712\n",
      "Epoch 14/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 28.5862 - val_loss: 24.8508\n",
      "Epoch 15/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 28.2395 - val_loss: 24.5040\n",
      "Epoch 16/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 27.9919 - val_loss: 24.2121\n",
      "Epoch 17/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 27.7771 - val_loss: 23.9648\n",
      "Epoch 18/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 27.6199 - val_loss: 23.7408\n",
      "Epoch 19/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 27.4508 - val_loss: 23.5983\n",
      "Epoch 20/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 27.2946 - val_loss: 23.4448\n",
      "Epoch 21/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 27.1487 - val_loss: 23.2900\n",
      "Epoch 22/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 27.0091 - val_loss: 23.1833\n",
      "Epoch 23/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 26.8658 - val_loss: 23.0421\n",
      "Epoch 24/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 26.7244 - val_loss: 22.9049\n",
      "Epoch 25/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 26.5780 - val_loss: 22.7895\n",
      "Epoch 26/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 26.4725 - val_loss: 22.6413\n",
      "Epoch 27/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 26.3169 - val_loss: 22.5312\n",
      "Epoch 28/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 26.1786 - val_loss: 22.4236\n",
      "Epoch 29/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 26.0569 - val_loss: 22.3543\n",
      "Epoch 30/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 25.9327 - val_loss: 22.2842\n",
      "Epoch 31/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 25.8144 - val_loss: 22.1639\n",
      "Epoch 32/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 25.7000 - val_loss: 22.0807\n",
      "Epoch 33/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 25.5793 - val_loss: 21.9645\n",
      "Epoch 34/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 25.4846 - val_loss: 21.9051\n",
      "Epoch 35/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 25.3634 - val_loss: 21.7732\n",
      "Epoch 36/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 25.2605 - val_loss: 21.6874\n",
      "Epoch 37/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 25.1427 - val_loss: 21.5682\n",
      "Epoch 38/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 25.0386 - val_loss: 21.4985\n",
      "Epoch 39/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 24.9384 - val_loss: 21.4293\n",
      "Epoch 40/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 24.8407 - val_loss: 21.3622\n",
      "Epoch 41/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 24.7406 - val_loss: 21.2811\n",
      "Epoch 42/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 24.6538 - val_loss: 21.1932\n",
      "Epoch 43/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 24.5490 - val_loss: 21.1161\n",
      "Epoch 44/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 24.4694 - val_loss: 21.0679\n",
      "Epoch 45/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 24.3770 - val_loss: 20.9896\n",
      "Epoch 46/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 24.3070 - val_loss: 20.8988\n",
      "Epoch 47/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 24.2244 - val_loss: 20.8753\n",
      "Epoch 48/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 24.1410 - val_loss: 20.7961\n",
      "Epoch 49/300\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 24.0681 - val_loss: 20.7258\n",
      "Epoch 50/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 24.0074 - val_loss: 20.6937\n",
      "Epoch 51/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 23.9254 - val_loss: 20.5972\n",
      "Epoch 52/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 23.8572 - val_loss: 20.5493\n",
      "Epoch 53/300\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 23.7911 - val_loss: 20.4691\n",
      "Epoch 54/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 23.7369 - val_loss: 20.4154\n",
      "Epoch 55/300\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 23.7139 - val_loss: 20.4381\n",
      "Epoch 56/300\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 23.5979 - val_loss: 20.3501\n",
      "Epoch 57/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 23.5396 - val_loss: 20.2900\n",
      "Epoch 58/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 23.4937 - val_loss: 20.2504\n",
      "Epoch 59/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 23.4301 - val_loss: 20.1871\n",
      "Epoch 60/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 23.3865 - val_loss: 20.1564\n",
      "Epoch 61/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 23.3298 - val_loss: 20.1344\n",
      "Epoch 62/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 23.2850 - val_loss: 20.1103\n",
      "Epoch 63/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 23.2345 - val_loss: 20.0871\n",
      "Epoch 64/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 23.1999 - val_loss: 20.0644\n",
      "Epoch 65/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 23.1681 - val_loss: 19.9921\n",
      "Epoch 66/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 23.1095 - val_loss: 19.9791\n",
      "Epoch 67/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 23.0569 - val_loss: 19.9342\n",
      "Epoch 68/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 23.0075 - val_loss: 19.9251\n",
      "Epoch 69/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.9709 - val_loss: 19.8873\n",
      "Epoch 70/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.9262 - val_loss: 19.8599\n",
      "Epoch 71/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.8969 - val_loss: 19.8343\n",
      "Epoch 72/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.8577 - val_loss: 19.8047\n",
      "Epoch 73/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.8522 - val_loss: 19.7565\n",
      "Epoch 74/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.7772 - val_loss: 19.7554\n",
      "Epoch 75/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.7502 - val_loss: 19.7534\n",
      "Epoch 76/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.7420 - val_loss: 19.7942\n",
      "Epoch 77/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.7067 - val_loss: 19.7381\n",
      "Epoch 78/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.6529 - val_loss: 19.6580\n",
      "Epoch 79/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.6273 - val_loss: 19.6446\n",
      "Epoch 80/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.5884 - val_loss: 19.6115\n",
      "Epoch 81/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 2ms/step - loss: 22.5671 - val_loss: 19.5712\n",
      "Epoch 82/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.5409 - val_loss: 19.5738\n",
      "Epoch 83/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.5123 - val_loss: 19.5828\n",
      "Epoch 84/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.4839 - val_loss: 19.5393\n",
      "Epoch 85/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.4646 - val_loss: 19.5343\n",
      "Epoch 86/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.4481 - val_loss: 19.4809\n",
      "Epoch 87/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.4197 - val_loss: 19.4798\n",
      "Epoch 88/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.3834 - val_loss: 19.4463\n",
      "Epoch 89/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.3653 - val_loss: 19.4254\n",
      "Epoch 90/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.3394 - val_loss: 19.3710\n",
      "Epoch 91/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.3330 - val_loss: 19.3553\n",
      "Epoch 92/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.3195 - val_loss: 19.3691\n",
      "Epoch 93/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.2738 - val_loss: 19.3335\n",
      "Epoch 94/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.2433 - val_loss: 19.3387\n",
      "Epoch 95/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.2367 - val_loss: 19.3702\n",
      "Epoch 96/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.2051 - val_loss: 19.3089\n",
      "Epoch 97/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.1881 - val_loss: 19.3085\n",
      "Epoch 98/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.1576 - val_loss: 19.2918\n",
      "Epoch 99/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.1414 - val_loss: 19.2748\n",
      "Epoch 100/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.1200 - val_loss: 19.2612\n",
      "Epoch 101/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.1076 - val_loss: 19.2201\n",
      "Epoch 102/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.0806 - val_loss: 19.2341\n",
      "Epoch 103/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.0744 - val_loss: 19.2525\n",
      "Epoch 104/300\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 22.0596 - val_loss: 19.2216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd425c07e50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model\n",
    "regression = Sequential()\n",
    "regression.add(Dense(1, input_shape=(X.shape[1],)))\n",
    "\n",
    "# Train the model\n",
    "regression.compile(optimizer='adam', loss='mean_squared_error')\n",
    "regression.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=300, batch_size=32, \n",
    "               callbacks=[EarlyStopping(patience=3)], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
